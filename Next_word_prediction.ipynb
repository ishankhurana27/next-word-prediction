{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODQuZS71y9Na9Q8E2YZz2K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishankhurana27/next-word-prediction/blob/main/Next_word_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIxe4owmmHe_",
        "outputId": "0bba392c-7c31-4297-fa96-38b7c182f7bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        }
      ],
      "source": [
        "# data collection\n",
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "import pandas as pd\n",
        "\n",
        "# load the dataset\n",
        "df=gutenberg.raw('shakespeare-hamlet.txt')\n",
        "\n",
        "#save to file\n",
        "with open('hamlet.txt','w') as f:\n",
        "  f.write(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data processing\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer  # convert text to vectors\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences # makes sure all sentences have same length while training lstm rnn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# load the dataset\n",
        "with open('hamlet.txt','r') as f:\n",
        "  text=f.read().lower()\n",
        "\n",
        "# tokenize the text-- creating index for words\n",
        "tokenizer=Tokenizer(num_words=5000,oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words=len(tokenizer.word_index)+1\n",
        "total_words\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es0GsEZQn2N6",
        "outputId": "56e2868e-b63c-4825-e3e6-2e733ffb4050"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4819"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_words=min(5000,len(tokenizer.word_index)+1)"
      ],
      "metadata": {
        "id": "baTxXpTArwTk"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create input sequence. converting every sentence into sequence of indexes\n",
        "input_sequences=[]\n",
        "for line in text.split('\\n'):\n",
        "  token_list=tokenizer.texts_to_sequences([line])[0]\n",
        "  for i in range(1,len(token_list)):\n",
        "    n_gram_sequence=token_list[:i+1]\n",
        "    input_sequences.append(n_gram_sequence)"
      ],
      "metadata": {
        "id": "XNwF7kVaperT"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdYcL7_wqXZO",
        "outputId": "60016c8e-6fb7-41f1-9f21-3ef34a0a2dbe"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 688],\n",
              " [2, 688, 5],\n",
              " [2, 688, 5, 46],\n",
              " [2, 688, 5, 46, 42],\n",
              " [2, 688, 5, 46, 42, 1887],\n",
              " [2, 688, 5, 46, 42, 1887, 1888],\n",
              " [2, 688, 5, 46, 42, 1887, 1888, 1889],\n",
              " [1181, 1890],\n",
              " [1181, 1890, 1891],\n",
              " [1181, 1890, 1891, 1892],\n",
              " [58, 408],\n",
              " [58, 408, 3],\n",
              " [58, 408, 3, 1182],\n",
              " [58, 408, 3, 1182, 178],\n",
              " [58, 408, 3, 1182, 178, 1893],\n",
              " [408, 1183],\n",
              " [408, 1183, 64],\n",
              " [409, 163],\n",
              " [409, 163, 378],\n",
              " [409, 163, 378, 22],\n",
              " [409, 163, 378, 22, 248],\n",
              " [409, 163, 378, 22, 248, 883],\n",
              " [19, 67],\n",
              " [452, 225],\n",
              " [452, 225, 249],\n",
              " [452, 225, 249, 2],\n",
              " [452, 225, 249, 2, 31],\n",
              " [409, 408],\n",
              " [452, 26],\n",
              " [409, 7],\n",
              " [409, 7, 44],\n",
              " [409, 7, 44, 63],\n",
              " [409, 7, 44, 63, 1894],\n",
              " [409, 7, 44, 63, 1894, 97],\n",
              " [409, 7, 44, 63, 1894, 97, 19],\n",
              " [409, 7, 44, 63, 1894, 97, 19, 567],\n",
              " [452, 72],\n",
              " [452, 72, 52],\n",
              " [452, 72, 52, 1895],\n",
              " [452, 72, 52, 1895, 568],\n",
              " [452, 72, 52, 1895, 568, 379],\n",
              " [452, 72, 52, 1895, 568, 379, 81],\n",
              " [452, 72, 52, 1895, 568, 379, 81, 4],\n",
              " [452, 72, 52, 1895, 568, 379, 81, 4, 274],\n",
              " [452, 72, 52, 1895, 568, 379, 81, 4, 274, 1182],\n",
              " [409, 21],\n",
              " [409, 21, 17],\n",
              " [409, 21, 17, 1896],\n",
              " [409, 21, 17, 1896, 115],\n",
              " [409, 21, 17, 1896, 115, 380],\n",
              " [409, 21, 17, 1896, 115, 380, 72],\n",
              " [409, 21, 17, 1896, 115, 380, 72, 884],\n",
              " [409, 21, 17, 1896, 115, 380, 72, 884, 492],\n",
              " [3, 6],\n",
              " [3, 6, 93],\n",
              " [3, 6, 93, 689],\n",
              " [3, 6, 93, 689, 59],\n",
              " [3, 6, 93, 689, 59, 145],\n",
              " [347, 30],\n",
              " [347, 30, 7],\n",
              " [347, 30, 7, 109],\n",
              " [347, 30, 7, 109, 569],\n",
              " [347, 30, 7, 109, 569, 885],\n",
              " [409, 15],\n",
              " [409, 15, 8],\n",
              " [409, 15, 8, 886],\n",
              " [409, 15, 8, 886, 1897],\n",
              " [347, 65],\n",
              " [347, 65, 381],\n",
              " [347, 65, 381, 38],\n",
              " [347, 65, 381, 38, 7],\n",
              " [347, 65, 381, 38, 7, 48],\n",
              " [347, 65, 381, 38, 7, 48, 690],\n",
              " [347, 65, 381, 38, 7, 48, 690, 121],\n",
              " [347, 65, 381, 38, 7, 48, 690, 121, 3],\n",
              " [348, 2],\n",
              " [348, 2, 1898],\n",
              " [348, 2, 1898, 5],\n",
              " [348, 2, 1898, 5, 9],\n",
              " [348, 2, 1898, 5, 9, 258],\n",
              " [348, 2, 1898, 5, 9, 258, 493],\n",
              " [348, 2, 1898, 5, 9, 258, 493, 69],\n",
              " [348, 2, 1898, 5, 9, 258, 493, 69, 88],\n",
              " [348, 2, 1898, 5, 9, 258, 493, 69, 88, 150],\n",
              " [58, 121],\n",
              " [58, 121, 3],\n",
              " [58, 121, 3, 348],\n",
              " [409, 6],\n",
              " [409, 6, 118],\n",
              " [409, 6, 118, 6],\n",
              " [409, 6, 118, 6, 140],\n",
              " [409, 6, 118, 6, 140, 69],\n",
              " [409, 6, 118, 6, 140, 69, 248],\n",
              " [409, 6, 118, 6, 140, 69, 248, 1183],\n",
              " [409, 6, 118, 6, 140, 69, 248, 1183, 64],\n",
              " [49, 206],\n",
              " [49, 206, 4],\n",
              " [49, 206, 4, 17],\n",
              " [49, 206, 4, 17, 410],\n",
              " [141, 3],\n",
              " [141, 3, 1899],\n",
              " [141, 3, 1899, 349],\n",
              " [141, 3, 1899, 349, 4],\n",
              " [141, 3, 1899, 349, 4, 2],\n",
              " [141, 3, 1899, 349, 4, 2, 494],\n",
              " [409, 80],\n",
              " [409, 80, 7],\n",
              " [409, 80, 7, 47],\n",
              " [409, 80, 7, 47, 125],\n",
              " [141, 126],\n",
              " [141, 126, 1900],\n",
              " [141, 126, 1900, 323],\n",
              " [141, 126, 1900, 323, 1184],\n",
              " [141, 126, 1900, 323, 1184, 122],\n",
              " [141, 126, 1900, 323, 1184, 122, 84],\n",
              " [141, 126, 1900, 323, 1184, 122, 84, 1901],\n",
              " [141, 126, 1900, 323, 1184, 122, 84, 1901, 7],\n",
              " [1902, 408],\n",
              " [1902, 408, 259],\n",
              " [1902, 408, 259, 9],\n",
              " [1902, 408, 259, 9, 495],\n",
              " [1902, 408, 259, 9, 495, 80],\n",
              " [1902, 408, 259, 9, 495, 80, 7],\n",
              " [1902, 408, 259, 9, 495, 80, 7, 381],\n",
              " [219, 409],\n",
              " [141, 1903],\n",
              " [141, 1903, 408],\n",
              " [452, 95],\n",
              " [452, 95, 25],\n",
              " [452, 95, 25, 14],\n",
              " [452, 95, 25, 14, 121],\n",
              " [452, 95, 25, 14, 121, 64],\n",
              " [49, 8],\n",
              " [49, 8, 496],\n",
              " [49, 8, 496, 5],\n",
              " [49, 8, 496, 5, 29],\n",
              " [452, 237],\n",
              " [452, 237, 121],\n",
              " [452, 237, 121, 237],\n",
              " [452, 237, 121, 237, 47],\n",
              " [452, 237, 121, 237, 47, 348],\n",
              " [141, 25],\n",
              " [141, 25, 259],\n",
              " [141, 25, 259, 17],\n",
              " [141, 25, 259, 17, 168],\n",
              " [141, 25, 259, 17, 168, 887],\n",
              " [141, 25, 259, 17, 168, 887, 132],\n",
              " [141, 25, 259, 17, 168, 887, 132, 4],\n",
              " [141, 25, 259, 17, 168, 887, 132, 4, 125],\n",
              " [452, 6],\n",
              " [452, 6, 30],\n",
              " [452, 6, 30, 190],\n",
              " [452, 6, 30, 190, 154],\n",
              " [141, 121],\n",
              " [141, 121, 888],\n",
              " [141, 121, 888, 72],\n",
              " [141, 121, 888, 72, 20],\n",
              " [141, 121, 888, 72, 20, 34],\n",
              " [141, 121, 888, 72, 20, 34, 1185],\n",
              " [3, 32],\n",
              " [3, 32, 15],\n",
              " [3, 32, 15, 51],\n",
              " [3, 32, 15, 51, 1904],\n",
              " [3, 32, 15, 51, 1904, 137],\n",
              " [3, 32, 15, 51, 1904, 137, 169],\n",
              " [3, 32, 15, 51, 1904, 137, 169, 5],\n",
              " [3, 32, 15, 51, 1904, 137, 169, 5, 29],\n",
              " [889, 17],\n",
              " [889, 17, 1905],\n",
              " [889, 17, 1905, 411],\n",
              " [889, 17, 1905, 411, 691],\n",
              " [889, 17, 1905, 411, 691, 190],\n",
              " [889, 17, 1905, 411, 691, 190, 5],\n",
              " [889, 17, 1905, 411, 691, 190, 5, 73],\n",
              " [260, 6],\n",
              " [260, 6, 30],\n",
              " [260, 6, 30, 1906],\n",
              " [260, 6, 30, 1906, 29],\n",
              " [260, 6, 30, 1906, 29, 890],\n",
              " [18, 73],\n",
              " [18, 73, 4],\n",
              " [18, 73, 4, 258],\n",
              " [18, 73, 4, 258, 2],\n",
              " [18, 73, 4, 258, 2, 1907],\n",
              " [18, 73, 4, 258, 2, 1907, 5],\n",
              " [18, 73, 4, 258, 2, 1907, 5, 17],\n",
              " [18, 73, 4, 258, 2, 1907, 5, 17, 125],\n",
              " [12, 38],\n",
              " [12, 38, 132],\n",
              " [12, 38, 132, 17],\n",
              " [12, 38, 132, 17, 1186],\n",
              " [12, 38, 132, 17, 1186, 44],\n",
              " [26, 71],\n",
              " [26, 71, 1908],\n",
              " [26, 71, 1908, 34],\n",
              " [26, 71, 1908, 34, 191],\n",
              " [26, 71, 1908, 34, 191, 3],\n",
              " [26, 71, 1908, 34, 191, 3, 86],\n",
              " [26, 71, 1908, 34, 191, 3, 86, 4],\n",
              " [26, 71, 1908, 34, 191, 3, 86, 4, 10],\n",
              " [49, 1187],\n",
              " [49, 1187, 1187],\n",
              " [49, 1187, 1187, 692],\n",
              " [49, 1187, 1187, 692, 15],\n",
              " [49, 1187, 1187, 692, 15, 891],\n",
              " [452, 412],\n",
              " [452, 412, 151],\n",
              " [452, 412, 151, 8],\n",
              " [452, 412, 151, 8, 238],\n",
              " [3, 51],\n",
              " [3, 51, 73],\n",
              " [3, 51, 73, 207],\n",
              " [3, 51, 73, 207, 132],\n",
              " [3, 51, 73, 207, 132, 1909],\n",
              " [3, 51, 73, 207, 132, 1909, 19],\n",
              " [3, 51, 73, 207, 132, 1909, 19, 382],\n",
              " [12, 37],\n",
              " [12, 37, 28],\n",
              " [12, 37, 28, 1910],\n",
              " [12, 37, 28, 1910, 192],\n",
              " [12, 37, 28, 1910, 192, 34],\n",
              " [12, 37, 28, 1910, 192, 34, 1188],\n",
              " [25, 35],\n",
              " [25, 35, 178],\n",
              " [25, 35, 178, 693],\n",
              " [25, 35, 178, 693, 30],\n",
              " [25, 35, 178, 693, 30, 190],\n",
              " [49, 65],\n",
              " [49, 65, 412],\n",
              " [49, 65, 412, 35],\n",
              " [49, 65, 412, 35, 151],\n",
              " [3, 51],\n",
              " [3, 51, 73],\n",
              " [3, 51, 73, 140],\n",
              " [3, 51, 73, 140, 408],\n",
              " [3, 51, 73, 140, 408, 86],\n",
              " [3, 51, 73, 140, 408, 86, 5],\n",
              " [3, 51, 73, 140, 408, 86, 5, 17],\n",
              " [347, 275],\n",
              " [347, 275, 125],\n",
              " [347, 275, 125, 5],\n",
              " [347, 275, 125, 5, 39],\n",
              " [96, 1911],\n",
              " [96, 1911, 276],\n",
              " [96, 1911, 276, 694],\n",
              " [96, 1911, 276, 694, 250],\n",
              " [96, 1911, 276, 694, 250, 1912],\n",
              " [96, 1911, 276, 694, 250, 1912, 57],\n",
              " [96, 1911, 276, 694, 250, 1912, 57, 2],\n",
              " [96, 1911, 276, 694, 250, 1912, 57, 2, 1189],\n",
              " [109, 148],\n",
              " [109, 148, 16],\n",
              " [109, 148, 16, 570],\n",
              " [109, 148, 16, 570, 695],\n",
              " [109, 148, 16, 570, 695, 1913],\n",
              " [109, 148, 16, 570, 695, 1913, 12],\n",
              " [109, 148, 16, 570, 695, 1913, 12, 220],\n",
              " [109, 148, 16, 570, 695, 1913, 12, 220, 5],\n",
              " [109, 148, 16, 570, 695, 1913, 12, 220, 5, 110],\n",
              " [90, 52],\n",
              " [90, 52, 10],\n",
              " [90, 52, 10, 1190],\n",
              " [90, 52, 10, 1190, 348],\n",
              " [90, 52, 10, 1190, 348, 3],\n",
              " [90, 52, 10, 1190, 348, 3, 9],\n",
              " [90, 52, 10, 1190, 348, 3, 9, 67],\n",
              " [2, 1191],\n",
              " [2, 1191, 40],\n",
              " [2, 1191, 40, 892],\n",
              " [2, 1191, 40, 892, 104],\n",
              " [141, 350],\n",
              " [141, 350, 383],\n",
              " [141, 350, 383, 81],\n",
              " [141, 350, 383, 81, 5],\n",
              " [58, 2],\n",
              " [58, 2, 184],\n",
              " [142, 90],\n",
              " [142, 90, 10],\n",
              " [142, 90, 10, 174],\n",
              " [142, 90, 10, 174, 132],\n",
              " [347, 11],\n",
              " [347, 11, 2],\n",
              " [347, 11, 2, 276],\n",
              " [347, 11, 2, 276, 696],\n",
              " [347, 11, 2, 276, 696, 62],\n",
              " [347, 11, 2, 276, 696, 62, 2],\n",
              " [347, 11, 2, 276, 696, 62, 2, 31],\n",
              " [347, 11, 2, 276, 696, 62, 2, 31, 250],\n",
              " [347, 11, 2, 276, 696, 62, 2, 31, 250, 146],\n",
              " [141, 43],\n",
              " [141, 43, 239],\n",
              " [141, 43, 239, 8],\n",
              " [141, 43, 239, 8, 1914],\n",
              " [141, 43, 239, 8, 1914, 86],\n",
              " [141, 43, 239, 8, 1914, 86, 4],\n",
              " [141, 43, 239, 8, 1914, 86, 4, 10],\n",
              " [141, 43, 239, 8, 1914, 86, 4, 10, 121],\n",
              " [347, 453],\n",
              " [347, 453, 10],\n",
              " [347, 453, 10, 15],\n",
              " [347, 453, 10, 15, 62],\n",
              " [347, 453, 10, 15, 62, 2],\n",
              " [347, 453, 10, 15, 62, 2, 31],\n",
              " [347, 453, 10, 15, 62, 2, 31, 277],\n",
              " [347, 453, 10, 15, 62, 2, 31, 277, 10],\n",
              " [347, 453, 10, 15, 62, 2, 31, 277, 10, 121],\n",
              " [384, 63],\n",
              " [384, 63, 62],\n",
              " [384, 63, 62, 10],\n",
              " [384, 63, 62, 10, 1915],\n",
              " [384, 63, 62, 10, 1915, 22],\n",
              " [384, 63, 62, 10, 1915, 22, 18],\n",
              " [384, 63, 62, 10, 1915, 22, 18, 1916],\n",
              " [384, 63, 62, 10, 1915, 22, 18, 1916, 697],\n",
              " [347, 10],\n",
              " [347, 10, 68],\n",
              " [347, 10, 68, 27],\n",
              " [347, 10, 68, 27, 1192],\n",
              " [347, 10, 68, 27, 1192, 91],\n",
              " [141, 261],\n",
              " [141, 261, 10],\n",
              " [141, 261, 10, 121],\n",
              " [49, 25],\n",
              " [49, 25, 239],\n",
              " [49, 25, 239, 43],\n",
              " [49, 25, 239, 43, 12],\n",
              " [49, 25, 239, 43, 12, 1917],\n",
              " [49, 25, 239, 43, 12, 1917, 17],\n",
              " [49, 25, 239, 43, 12, 1917, 17, 111],\n",
              " [49, 25, 239, 43, 12, 1917, 17, 111, 5],\n",
              " [49, 25, 239, 43, 12, 1917, 17, 111, 5, 125],\n",
              " [351, 18],\n",
              " [351, 18, 12],\n",
              " [351, 18, 12, 193],\n",
              " [351, 18, 12, 193, 3],\n",
              " [351, 18, 12, 193, 3, 698],\n",
              " [351, 18, 12, 193, 3, 698, 302],\n",
              " [11, 76],\n",
              " [11, 76, 2],\n",
              " [11, 76, 2, 413],\n",
              " [11, 76, 2, 413, 5],\n",
              " [11, 76, 2, 413, 5, 497],\n",
              " [11, 76, 2, 413, 5, 497, 194],\n",
              " [77, 571],\n",
              " [77, 571, 572],\n",
              " [77, 571, 572, 42],\n",
              " [77, 571, 572, 42, 110],\n",
              " [77, 571, 572, 42, 110, 6],\n",
              " [77, 571, 572, 42, 110, 6, 454],\n",
              " [77, 571, 572, 42, 110, 6, 454, 81],\n",
              " [77, 571, 572, 42, 110, 6, 454, 81, 86],\n",
              " [141, 10],\n",
              " [141, 10, 14],\n",
              " [141, 10, 14, 893],\n",
              " [347, 106],\n",
              " [347, 106, 10],\n",
              " [347, 106, 10, 1918],\n",
              " [347, 106, 10, 1918, 155],\n",
              " [49, 324],\n",
              " [49, 324, 86],\n",
              " [49, 324, 86, 86],\n",
              " [49, 324, 86, 86, 6],\n",
              " [49, 324, 86, 86, 6, 454],\n",
              " [49, 324, 86, 86, 6, 454, 81],\n",
              " [49, 324, 86, 86, 6, 454, 81, 86],\n",
              " [219, 2],\n",
              " [219, 2, 184],\n",
              " [141, 72],\n",
              " [141, 72, 278],\n",
              " [141, 72, 278, 3],\n",
              " [141, 72, 278, 3, 32],\n",
              " [141, 72, 278, 3, 32, 15],\n",
              " [141, 72, 278, 3, 32, 15, 378],\n",
              " [347, 54],\n",
              " [347, 54, 52],\n",
              " [347, 54, 52, 121],\n",
              " [347, 54, 52, 121, 7],\n",
              " [347, 54, 52, 121, 7, 1193],\n",
              " [347, 54, 52, 121, 7, 1193, 573],\n",
              " [347, 54, 52, 121, 7, 1193, 573, 414],\n",
              " [14, 15],\n",
              " [14, 15, 17],\n",
              " [14, 15, 17, 303],\n",
              " [14, 15, 17, 303, 55],\n",
              " [14, 15, 17, 303, 55, 40],\n",
              " [14, 15, 17, 303, 55, 40, 1185],\n",
              " [25, 118],\n",
              " [25, 118, 7],\n",
              " [25, 118, 7, 415],\n",
              " [49, 226],\n",
              " [49, 226, 9],\n",
              " [49, 226, 9, 164],\n",
              " [49, 226, 9, 164, 6],\n",
              " [49, 226, 9, 164, 6, 147],\n",
              " [49, 226, 9, 164, 6, 147, 15],\n",
              " [49, 226, 9, 164, 6, 147, 15, 17],\n",
              " [49, 226, 9, 164, 6, 147, 15, 17, 227],\n",
              " [279, 2],\n",
              " [279, 2, 1194],\n",
              " [279, 2, 1194, 3],\n",
              " [279, 2, 1194, 3, 179],\n",
              " [279, 2, 1194, 3, 179, 1919],\n",
              " [5, 112],\n",
              " [5, 112, 143],\n",
              " [5, 112, 143, 191],\n",
              " [141, 14],\n",
              " [141, 14, 10],\n",
              " [141, 14, 10, 15],\n",
              " [141, 14, 10, 15, 62],\n",
              " [141, 14, 10, 15, 62, 2],\n",
              " [141, 14, 10, 15, 62, 2, 31],\n",
              " [49, 24],\n",
              " [49, 24, 43],\n",
              " [49, 24, 43, 239],\n",
              " [49, 24, 43, 239, 4],\n",
              " [49, 24, 43, 239, 4, 53],\n",
              " [49, 24, 43, 239, 4, 53, 67],\n",
              " [94, 60],\n",
              " [94, 60, 2],\n",
              " [94, 60, 2, 100],\n",
              " [94, 60, 2, 100, 1195],\n",
              " [94, 60, 2, 100, 1195, 26],\n",
              " [94, 60, 2, 100, 1195, 26, 109],\n",
              " [94, 60, 2, 100, 1195, 26, 109, 36],\n",
              " [96, 135],\n",
              " [96, 135, 894],\n",
              " [96, 135, 894, 574],\n",
              " [96, 135, 894, 574, 1920],\n",
              " [28, 1921],\n",
              " [28, 1921, 26],\n",
              " [28, 1921, 26, 207],\n",
              " [28, 1921, 26, 207, 96],\n",
              " [28, 1921, 26, 207, 96, 11],\n",
              " [28, 1921, 26, 207, 96, 11, 92],\n",
              " [28, 1921, 26, 207, 96, 11, 92, 1922],\n",
              " [28, 1921, 26, 207, 96, 11, 92, 1922, 1923],\n",
              " [26, 1924],\n",
              " [26, 1924, 2],\n",
              " [26, 1924, 2, 1925],\n",
              " [26, 1924, 2, 1925, 1926],\n",
              " [26, 1924, 2, 1925, 1926, 36],\n",
              " [26, 1924, 2, 1925, 1926, 36, 2],\n",
              " [26, 1924, 2, 1925, 1926, 36, 2, 1196],\n",
              " [72, 385],\n",
              " [141, 120],\n",
              " [141, 120, 691],\n",
              " [141, 120, 691, 226],\n",
              " [141, 120, 691, 226, 3],\n",
              " [141, 120, 691, 226, 3, 895],\n",
              " [141, 120, 691, 226, 3, 895, 59],\n",
              " [141, 120, 691, 226, 3, 895, 59, 17],\n",
              " [141, 120, 691, 226, 3, 895, 59, 17, 146],\n",
              " [141, 120, 691, 226, 3, 895, 59, 17, 146, 567],\n",
              " [18, 1927],\n",
              " [18, 1927, 1928],\n",
              " [18, 1927, 1928, 84],\n",
              " [18, 1927, 1928, 84, 26],\n",
              " [18, 1927, 1928, 84, 26, 278],\n",
              " [18, 1927, 1928, 84, 26, 278, 42],\n",
              " [18, 1927, 1928, 84, 26, 278, 42, 34],\n",
              " [18, 1927, 1928, 84, 26, 278, 42, 34, 258],\n",
              " [49, 11],\n",
              " [49, 11, 25],\n",
              " [49, 11, 25, 575],\n",
              " [49, 11, 25, 575, 262],\n",
              " [49, 11, 25, 575, 262, 4],\n",
              " [49, 11, 25, 575, 262, 4, 1929],\n",
              " [49, 11, 25, 575, 262, 4, 1929, 6],\n",
              " [49, 11, 25, 575, 262, 4, 1929, 6, 66],\n",
              " [49, 11, 25, 575, 262, 4, 1929, 6, 66, 15],\n",
              " [20, 11],\n",
              " [20, 11, 2],\n",
              " [20, 11, 2, 1197],\n",
              " [20, 11, 2, 1197, 3],\n",
              " [20, 11, 2, 1197, 3, 1198],\n",
              " [20, 11, 2, 1197, 3, 1198, 5],\n",
              " [20, 11, 2, 1197, 3, 1198, 5, 9],\n",
              " [20, 11, 2, 1197, 3, 1198, 5, 9, 1930],\n",
              " [17, 1931],\n",
              " [17, 1931, 107],\n",
              " [17, 1931, 107, 385],\n",
              " [17, 1931, 107, 385, 1932],\n",
              " [17, 1931, 107, 385, 1932, 4],\n",
              " [17, 1931, 107, 385, 1932, 4, 34],\n",
              " [17, 1931, 107, 385, 1932, 4, 34, 280],\n",
              " [141, 47],\n",
              " [141, 47, 52],\n",
              " [141, 47, 52, 412],\n",
              " [141, 47, 52, 412, 151],\n",
              " [141, 47, 52, 412, 151, 116],\n",
              " [141, 47, 52, 412, 151, 116, 22],\n",
              " [141, 47, 52, 412, 151, 116, 22, 26],\n",
              " [141, 47, 52, 412, 151, 116, 22, 26, 12],\n",
              " [141, 47, 52, 412, 151, 116, 22, 26, 12, 896],\n",
              " [78, 17],\n",
              " [78, 17, 276],\n",
              " [78, 17, 276, 1933],\n",
              " [78, 17, 276, 1933, 3],\n",
              " [78, 17, 276, 1933, 3, 63],\n",
              " [78, 17, 276, 1933, 3, 63, 1934],\n",
              " [78, 17, 276, 1933, 3, 63, 1934, 258],\n",
              " [28, 1199],\n",
              " [28, 1199, 1935],\n",
              " [28, 1199, 1935, 2],\n",
              " [28, 1199, 1935, 2, 897],\n",
              " [28, 1199, 1935, 2, 897, 5],\n",
              " [28, 1199, 1935, 2, 897, 5, 2],\n",
              " [28, 1199, 1935, 2, 897, 5, 2, 576],\n",
              " [3, 78],\n",
              " [3, 78, 94],\n",
              " [3, 78, 94, 1936],\n",
              " [3, 78, 94, 1936, 498],\n",
              " [3, 78, 94, 1936, 498, 5],\n",
              " [3, 78, 94, 1936, 498, 5, 1937],\n",
              " [3, 78, 94, 1936, 498, 5, 1937, 699],\n",
              " [3, 1938],\n",
              " [3, 1938, 1939],\n",
              " [3, 1938, 1939, 21],\n",
              " [3, 1938, 1939, 21, 1940],\n",
              " [3, 1938, 1939, 21, 1940, 5],\n",
              " [3, 1938, 1939, 21, 1940, 5, 1200],\n",
              " [78, 94],\n",
              " [78, 94, 1941],\n",
              " [78, 94, 1941, 5],\n",
              " [78, 94, 1941, 5, 1201],\n",
              " [78, 94, 1941, 5, 1201, 1942],\n",
              " [78, 94, 1941, 5, 1201, 1942, 156],\n",
              " [78, 94, 1941, 5, 1201, 1942, 156, 898],\n",
              " [78, 94, 1941, 5, 1201, 1942, 156, 898, 1943],\n",
              " [577, 15],\n",
              " [577, 15, 1944],\n",
              " [577, 15, 1944, 2],\n",
              " [577, 15, 1944, 2, 1945],\n",
              " [577, 15, 1944, 2, 1945, 57],\n",
              " [577, 15, 1944, 2, 1945, 57, 2],\n",
              " [577, 15, 1944, 2, 1945, 57, 2, 1946],\n",
              " [25, 147],\n",
              " [25, 147, 27],\n",
              " [25, 147, 27, 700],\n",
              " [25, 147, 27, 700, 12],\n",
              " [25, 147, 27, 700, 12, 17],\n",
              " [25, 147, 27, 700, 12, 17, 1947],\n",
              " [25, 147, 27, 700, 12, 17, 1947, 150],\n",
              " [157, 88],\n",
              " [157, 88, 2],\n",
              " [157, 88, 2, 125],\n",
              " [157, 88, 2, 125, 1202],\n",
              " [157, 88, 2, 125, 1202, 1948],\n",
              " [157, 88, 2, 125, 1202, 1948, 18],\n",
              " [157, 88, 2, 125, 1202, 1948, 18, 2],\n",
              " [157, 88, 2, 125, 1202, 1948, 18, 2, 170],\n",
              " [122, 281],\n",
              " [122, 281, 12],\n",
              " [122, 281, 12, 133],\n",
              " [122, 281, 12, 133, 1949],\n",
              " [122, 281, 12, 133, 1949, 22],\n",
              " [49, 12],\n",
              " [49, 12, 133],\n",
              " [49, 12, 133, 6],\n",
              " [59, 899],\n",
              " [59, 899, 2],\n",
              " [59, 899, 2, 1950],\n",
              " [59, 899, 2, 1950, 282],\n",
              " [59, 899, 2, 1950, 282, 28],\n",
              " [59, 899, 2, 1950, 282, 28, 34],\n",
              " [59, 899, 2, 1950, 282, 28, 34, 275],\n",
              " [59, 899, 2, 1950, 282, 28, 34, 275, 31],\n",
              " [156, 701],\n",
              " [156, 701, 185],\n",
              " [156, 701, 185, 20],\n",
              " [156, 701, 185, 20, 52],\n",
              " [156, 701, 185, 20, 52, 887],\n",
              " [156, 701, 185, 20, 52, 887, 4],\n",
              " [156, 701, 185, 20, 52, 887, 4, 73],\n",
              " [60, 24],\n",
              " [60, 24, 7],\n",
              " [60, 24, 7, 66],\n",
              " [60, 24, 7, 66, 42],\n",
              " [60, 24, 7, 66, 42, 283],\n",
              " [60, 24, 7, 66, 42, 283, 5],\n",
              " [60, 24, 7, 66, 42, 283, 5, 702],\n",
              " [1951, 1952],\n",
              " [1951, 1952, 36],\n",
              " [1951, 1952, 36, 42],\n",
              " [1951, 1952, 36, 42, 8],\n",
              " [1951, 1952, 36, 42, 8, 63],\n",
              " [1951, 1952, 36, 42, 8, 63, 1953],\n",
              " [1951, 1952, 36, 42, 8, 63, 1953, 1954],\n",
              " [1955, 4],\n",
              " [1955, 4, 2],\n",
              " [1955, 4, 2, 1956],\n",
              " [1955, 4, 2, 1956, 11],\n",
              " [1955, 4, 2, 1956, 11, 76],\n",
              " [1955, 4, 2, 1956, 11, 76, 34],\n",
              " [1955, 4, 2, 1956, 11, 76, 34, 900],\n",
              " [1955, 4, 2, 1956, 11, 76, 34, 900, 46],\n",
              " [21, 28],\n",
              " [21, 28, 17],\n",
              " [21, 28, 17, 1203],\n",
              " [21, 28, 17, 1203, 5],\n",
              " [21, 28, 17, 1203, 5, 34],\n",
              " [21, 28, 17, 1203, 5, 34, 455],\n",
              " [21, 28, 17, 1203, 5, 34, 455, 171],\n",
              " [21, 28, 17, 1203, 5, 34, 455, 171, 1957],\n",
              " [21, 28, 17, 1203, 5, 34, 455, 171, 1957, 29],\n",
              " [77, 1958],\n",
              " [77, 1958, 17],\n",
              " [77, 1958, 17, 283],\n",
              " [77, 1958, 17, 283, 122],\n",
              " [77, 1958, 17, 283, 122, 42],\n",
              " [77, 1958, 17, 283, 122, 42, 8],\n",
              " [77, 1958, 17, 283, 122, 42, 8, 703],\n",
              " [77, 1958, 17, 283, 122, 42, 8, 703, 1959],\n",
              " [65, 1960],\n",
              " [65, 1960, 42],\n",
              " [65, 1960, 42, 456],\n",
              " [65, 1960, 42, 456, 3],\n",
              " [65, 1960, 42, 456, 3, 1961],\n",
              " [77, 1962],\n",
              " [77, 1962, 18],\n",
              " [77, 1962, 18, 16],\n",
              " [77, 1962, 18, 16, 138],\n",
              " [77, 1962, 18, 16, 138, 39],\n",
              " [77, 1962, 18, 16, 138, 39, 195],\n",
              " [77, 1962, 18, 16, 138, 39, 195, 16],\n",
              " [77, 1962, 18, 16, 138, 39, 195, 16, 704],\n",
              " [76, 26],\n",
              " [76, 26, 705],\n",
              " [76, 26, 705, 1963],\n",
              " [76, 26, 705, 1963, 36],\n",
              " [76, 26, 705, 1963, 36, 4],\n",
              " [76, 26, 705, 1963, 36, 4, 2],\n",
              " [76, 26, 705, 1963, 36, 4, 2, 1964],\n",
              " [192, 2],\n",
              " [192, 2, 76],\n",
              " [192, 2, 76, 8],\n",
              " [192, 2, 76, 8, 1965],\n",
              " [192, 2, 76, 8, 1965, 1966],\n",
              " [60, 1967],\n",
              " [60, 1967, 42],\n",
              " [60, 1967, 42, 34],\n",
              " [60, 1967, 42, 34, 31],\n",
              " [60, 1967, 42, 34, 31, 76],\n",
              " [60, 1967, 42, 34, 31, 76, 109],\n",
              " [60, 1967, 42, 34, 31, 76, 109, 706],\n",
              " [4, 2],\n",
              " [4, 2, 1968],\n",
              " [4, 2, 1968, 5],\n",
              " [4, 2, 1968, 5, 283],\n",
              " [109, 26],\n",
              " [109, 26, 457],\n",
              " [109, 26, 457, 1969],\n",
              " [109, 26, 457, 1969, 24],\n",
              " [109, 26, 457, 1969, 24, 42],\n",
              " [109, 26, 457, 1969, 24, 42, 2],\n",
              " [109, 26, 457, 1969, 24, 42, 2, 276],\n",
              " [109, 26, 457, 1969, 24, 42, 2, 276, 1970],\n",
              " [3, 1971],\n",
              " [3, 1971, 5],\n",
              " [3, 1971, 5, 2],\n",
              " [3, 1971, 5, 2, 1972],\n",
              " [3, 1971, 5, 2, 1972, 1973],\n",
              " [16, 416],\n",
              " [16, 416, 4],\n",
              " [16, 416, 4, 46],\n",
              " [16, 416, 4, 46, 52],\n",
              " [16, 416, 4, 46, 52, 74],\n",
              " [16, 416, 4, 46, 52, 74, 386],\n",
              " [16, 416, 4, 46, 52, 74, 386, 283],\n",
              " [5, 1974],\n",
              " [5, 1974, 1204],\n",
              " [5, 1974, 1204, 499],\n",
              " [5, 1974, 1204, 499, 3],\n",
              " [5, 1974, 1204, 499, 3, 352],\n",
              " [84, 11],\n",
              " [84, 11, 2],\n",
              " [84, 11, 2, 1975],\n",
              " [84, 11, 2, 1975, 5],\n",
              " [84, 11, 2, 1975, 5, 702],\n",
              " [84, 11, 2, 1975, 5, 702, 108],\n",
              " [84, 11, 2, 1975, 5, 702, 108, 3],\n",
              " [84, 11, 2, 1975, 5, 702, 108, 3, 64],\n",
              " [1976, 129],\n",
              " [1976, 129, 8],\n",
              " [1976, 129, 8, 500],\n",
              " [1976, 129, 8, 500, 5],\n",
              " [1976, 129, 8, 500, 5, 1977],\n",
              " [1976, 129, 8, 500, 5, 1977, 1978],\n",
              " [21, 1979],\n",
              " [21, 1979, 3],\n",
              " [21, 1979, 3, 1205],\n",
              " [21, 1979, 3, 1205, 4],\n",
              " [21, 1979, 3, 1205, 4, 107],\n",
              " [21, 1979, 3, 1205, 4, 107, 1206],\n",
              " [12, 84],\n",
              " [12, 84, 8],\n",
              " [12, 84, 8, 1980],\n",
              " [12, 84, 8, 1980, 325],\n",
              " [12, 84, 8, 1980, 325, 76],\n",
              " [12, 84, 8, 1980, 325, 76, 14],\n",
              " [12, 84, 8, 1980, 325, 76, 14, 33],\n",
              " [12, 84, 8, 1980, 325, 76, 14, 33, 158],\n",
              " [3, 10],\n",
              " [3, 10, 157],\n",
              " [3, 10, 157, 65],\n",
              " [3, 10, 157, 65, 891],\n",
              " [3, 10, 157, 65, 891, 578],\n",
              " [3, 10, 157, 65, 891, 578, 34],\n",
              " [3, 10, 157, 65, 891, 578, 34, 280],\n",
              " [20, 4],\n",
              " [20, 4, 901],\n",
              " [20, 4, 901, 5],\n",
              " [20, 4, 901, 5, 73],\n",
              " [20, 4, 901, 5, 73, 42],\n",
              " [20, 4, 901, 5, 73, 42, 707],\n",
              " [20, 4, 901, 5, 73, 42, 707, 208],\n",
              " [3, 902],\n",
              " [3, 902, 1981],\n",
              " [3, 902, 1981, 195],\n",
              " [3, 902, 1981, 195, 1982],\n",
              " [3, 902, 1981, 195, 1982, 704],\n",
              " [28, 42],\n",
              " [28, 42, 16],\n",
              " [28, 42, 16, 98],\n",
              " [28, 42, 16, 98, 417],\n",
              " [28, 42, 16, 98, 417, 3],\n",
              " [28, 42, 16, 98, 417, 3, 17],\n",
              " [28, 42, 16, 98, 417, 3, 17, 6],\n",
              " [28, 42, 16, 98, 417, 3, 17, 6, 137],\n",
              " [28, 42, 16, 98, 417, 3, 17, 6, 137, 10],\n",
              " [14, 2],\n",
              " [14, 2, 903],\n",
              " [14, 2, 903, 708],\n",
              " [14, 2, 903, 708, 5],\n",
              " [14, 2, 903, 708, 5, 34],\n",
              " [14, 2, 903, 708, 5, 34, 1983],\n",
              " [2, 1207],\n",
              " [2, 1207, 5],\n",
              " [2, 1207, 5, 17],\n",
              " [2, 1207, 5, 17, 34],\n",
              " [2, 1207, 5, 17, 34, 258],\n",
              " [2, 1207, 5, 17, 34, 258, 3],\n",
              " [2, 1207, 5, 17, 34, 258, 3, 2],\n",
              " [2, 1207, 5, 17, 34, 258, 3, 2, 1208],\n",
              " [2, 1207, 5, 17, 34, 258, 3, 2, 1208, 165],\n",
              " [5, 17],\n",
              " [5, 17, 1209],\n",
              " [5, 17, 1209, 150],\n",
              " [5, 17, 1209, 150, 3],\n",
              " [5, 17, 1209, 150, 3, 1984],\n",
              " [5, 17, 1209, 150, 3, 1984, 11],\n",
              " [5, 17, 1209, 150, 3, 1984, 11, 2],\n",
              " [5, 17, 1209, 150, 3, 1984, 11, 2, 576],\n",
              " [58, 184],\n",
              " [58, 184, 132],\n",
              " [20, 418],\n",
              " [20, 418, 1985],\n",
              " [20, 418, 1985, 904],\n",
              " [20, 418, 1985, 904, 90],\n",
              " [20, 418, 1985, 904, 90, 10],\n",
              " [20, 418, 1985, 904, 90, 10, 174],\n",
              " [20, 418, 1985, 904, 90, 10, 174, 132],\n",
              " [82, 1210],\n",
              " [82, 1210, 10],\n",
              " [82, 1210, 10, 180],\n",
              " [82, 1210, 10, 180, 10],\n",
              " [82, 1210, 10, 180, 10, 1211],\n",
              " [82, 1210, 10, 180, 10, 1211, 22],\n",
              " [82, 1210, 10, 180, 10, 1211, 22, 324],\n",
              " [82, 1210, 10, 180, 10, 1211, 22, 324, 1986],\n",
              " [38, 43],\n",
              " [38, 43, 150],\n",
              " [38, 43, 150, 228],\n",
              " [38, 43, 150, 228, 458],\n",
              " [38, 43, 150, 228, 458, 45],\n",
              " [38, 43, 150, 228, 458, 45, 284],\n",
              " [38, 43, 150, 228, 458, 45, 284, 5],\n",
              " [38, 43, 150, 228, 458, 45, 284, 5, 353],\n",
              " [86, 4],\n",
              " [86, 4, 22],\n",
              " [86, 4, 22, 38],\n",
              " [86, 4, 22, 38, 64],\n",
              " [86, 4, 22, 38, 64, 27],\n",
              " [86, 4, 22, 38, 64, 27, 228],\n",
              " [86, 4, 22, 38, 64, 27, 228, 47],\n",
              " [86, 4, 22, 38, 64, 27, 228, 47, 168],\n",
              " [86, 4, 22, 38, 64, 27, 228, 47, 168, 4],\n",
              " [86, 4, 22, 38, 64, 27, 228, 47, 168, 4, 27],\n",
              " [86, 4, 22, 38, 64, 27, 228, 47, 168, 4, 27, 175],\n",
              " [12, 71],\n",
              " [12, 71, 4],\n",
              " [12, 71, 4, 81],\n",
              " [12, 71, 4, 81, 48],\n",
              " [12, 71, 4, 81, 48, 709],\n",
              " [12, 71, 4, 81, 48, 709, 3],\n",
              " [12, 71, 4, 81, 48, 709, 3, 285],\n",
              " [12, 71, 4, 81, 48, 709, 3, 285, 4],\n",
              " [12, 71, 4, 81, 48, 709, 3, 285, 4, 22],\n",
              " [12, 71, 4, 81, 48, 709, 3, 285, 4, 22, 579],\n",
              " [12, 71, 4, 81, 48, 709, 3, 285, 4, 22, 579, 4],\n",
              " [12, 71, 4, 81, 48, 709, 3, 285, 4, 22, 579, 4, 22],\n",
              " [38, 43],\n",
              " [38, 43, 239],\n",
              " [38, 43, 239, 1987],\n",
              " [38, 43, 239, 1987, 4],\n",
              " [38, 43, 239, 1987, 4, 53],\n",
              " [38, 43, 239, 1987, 4, 53, 1212],\n",
              " [38, 43, 239, 1987, 4, 53, 1212, 1213],\n",
              " [76, 1214],\n",
              " [76, 1214, 1988],\n",
              " [76, 1214, 1988, 71],\n",
              " [76, 1214, 1988, 71, 1215],\n",
              " [76, 1214, 1988, 71, 1215, 61],\n",
              " [76, 1214, 1988, 71, 1215, 61, 86],\n",
              " [45, 38],\n",
              " [45, 38, 43],\n",
              " [45, 38, 43, 150],\n",
              " [45, 38, 43, 150, 129],\n",
              " [45, 38, 43, 150, 129, 1989],\n",
              " [45, 38, 43, 150, 129, 1989, 11],\n",
              " [45, 38, 43, 150, 129, 1989, 11, 53],\n",
              " [45, 38, 43, 150, 129, 1989, 11, 53, 138],\n",
              " [1990, 710],\n",
              " [1990, 710, 11],\n",
              " [1990, 710, 11, 2],\n",
              " [1990, 710, 11, 2, 1991],\n",
              " [1990, 710, 11, 2, 1991, 5],\n",
              " [1990, 710, 11, 2, 1991, 5, 186],\n",
              " [21, 76],\n",
              " [21, 76, 56],\n",
              " [21, 76, 56, 95],\n",
              " [21, 76, 56, 95, 7],\n",
              " [21, 76, 56, 95, 7, 580],\n",
              " [21, 76, 56, 95, 7, 580, 354],\n",
              " [21, 76, 56, 95, 7, 580, 354, 387],\n",
              " [21, 76, 56, 95, 7, 580, 354, 387, 11],\n",
              " [21, 76, 56, 95, 7, 580, 354, 387, 11, 130],\n",
              " [86, 5],\n",
              " [86, 5, 10],\n",
              " [86, 5, 10, 324],\n",
              " [86, 5, 10, 324, 3],\n",
              " [86, 5, 10, 324, 3, 86],\n",
              " [86, 5, 10, 324, 3, 86, 711],\n",
              " [86, 5, 10, 324, 3, 86, 711, 10],\n",
              " [86, 5, 10, 324, 3, 86, 711, 10, 348],\n",
              " [141, 41],\n",
              " [141, 41, 6],\n",
              " [141, 41, 6, 1216],\n",
              " [141, 41, 6, 1216, 59],\n",
              " [141, 41, 6, 1216, 59, 10],\n",
              " [141, 41, 6, 1216, 59, 10, 18],\n",
              " [141, 41, 6, 1216, 59, 10, 18, 9],\n",
              " [141, 41, 6, 1216, 59, 10, 18, 9, 1992],\n",
              " [49, 48],\n",
              " [49, 48, 38],\n",
              " [49, 48, 38, 10],\n",
              " [49, 48, 38, 10, 32],\n",
              " [49, 48, 38, 10, 32, 15],\n",
              " [49, 48, 38, 10, 32, 15, 248],\n",
              " [347, 72],\n",
              " [347, 72, 108],\n",
              " [49, 72],\n",
              " [49, 72, 108],\n",
              " [141, 72],\n",
              " [141, 72, 278],\n",
              " [219, 184],\n",
              " [35, 48],\n",
              " [35, 48, 10],\n",
              " [35, 48, 10, 501],\n",
              " [35, 48, 10, 501, 326],\n",
              " [35, 48, 10, 501, 326, 28],\n",
              " [35, 48, 10, 501, 326, 28, 1217],\n",
              " [4, 905],\n",
              " [4, 905, 10],\n",
              " [4, 905, 10, 2],\n",
              " [4, 905, 10, 2, 209],\n",
              " [4, 905, 10, 2, 209, 5],\n",
              " [4, 905, 10, 2, 209, 5, 712],\n",
              " [21, 10],\n",
              " [21, 10, 14],\n",
              " [21, 10, 14, 24],\n",
              " [21, 10, 14, 24, 2],\n",
              " [21, 10, 14, 24, 2, 304],\n",
              " [21, 10, 14, 24, 2, 304, 1993],\n",
              " [3, 34],\n",
              " [3, 34, 1994],\n",
              " [3, 34, 1994, 1218],\n",
              " [3, 34, 1994, 1218, 1219],\n",
              " [3, 34, 1994, 1218, 1219, 1995],\n",
              " [347, 10],\n",
              " [347, 10, 60],\n",
              " [347, 10, 60, 210],\n",
              " [347, 10, 60, 210, 4],\n",
              " [347, 10, 60, 210, 4, 86],\n",
              " [347, 10, 60, 210, 4, 86, 96],\n",
              " [347, 10, 60, 210, 4, 86, 96, 2],\n",
              " [347, 10, 60, 210, 4, 86, 96, 2, 581],\n",
              " [347, 10, 60, 210, 4, 86, 96, 2, 581, 1220],\n",
              " [49, 3],\n",
              " [49, 3, 40],\n",
              " [49, 3, 40, 10],\n",
              " [49, 3, 40, 10, 1996],\n",
              " [49, 3, 40, 10, 1996, 62],\n",
              " [49, 3, 40, 10, 1996, 62, 8],\n",
              " [49, 3, 40, 10, 1996, 62, 8, 582],\n",
              " [49, 3, 40, 10, 1996, 62, 8, 582, 168],\n",
              " [97, 8],\n",
              " [97, 8, 1997],\n",
              " [97, 8, 1997, 1998],\n",
              " [97, 8, 1997, 1998, 6],\n",
              " [97, 8, 1997, 1998, 6, 30],\n",
              " [97, 8, 1997, 1998, 6, 30, 263],\n",
              " [2, 581],\n",
              " [2, 581, 12],\n",
              " [2, 581, 12, 14],\n",
              " [2, 581, 12, 14, 2],\n",
              " [2, 581, 12, 14, 2, 713],\n",
              " [2, 581, 12, 14, 2, 713, 4],\n",
              " [2, 581, 12, 14, 2, 713, 4, 2],\n",
              " [2, 581, 12, 14, 2, 713, 4, 2, 170],\n",
              " [157, 18],\n",
              " [157, 18, 16],\n",
              " [157, 18, 16, 1999],\n",
              " [157, 18, 16, 1999, 3],\n",
              " [157, 18, 16, 1999, 3, 2000],\n",
              " [157, 18, 16, 1999, 3, 2000, 2001],\n",
              " [157, 18, 16, 1999, 3, 2000, 2001, 1221],\n",
              " [1222, 2],\n",
              " [1222, 2, 164],\n",
              " [1222, 2, 164, 5],\n",
              " [1222, 2, 164, 5, 170],\n",
              " [1222, 2, 164, 5, 170, 3],\n",
              " [1222, 2, 164, 5, 170, 3, 59],\n",
              " [1222, 2, 164, 5, 170, 3, 59, 16],\n",
              " [1222, 2, 164, 5, 170, 3, 59, 16, 2002],\n",
              " [714, 11],\n",
              " [714, 11, 502],\n",
              " [714, 11, 502, 45],\n",
              " [714, 11, 502, 45, 327],\n",
              " [714, 11, 502, 45, 327, 11],\n",
              " [714, 11, 502, 45, 327, 11, 186],\n",
              " [714, 11, 502, 45, 327, 11, 186, 45],\n",
              " [714, 11, 502, 45, 327, 11, 186, 45, 304],\n",
              " [135, 2003],\n",
              " [135, 2003, 3],\n",
              " [135, 2003, 3, 2004],\n",
              " [135, 2003, 3, 2004, 264],\n",
              " [135, 2003, 3, 2004, 264, 2005],\n",
              " [4, 16],\n",
              " [4, 16, 1223],\n",
              " [4, 16, 1223, 3],\n",
              " [4, 16, 1223, 3, 5],\n",
              " [4, 16, 1223, 3, 5, 2],\n",
              " [4, 16, 1223, 3, 5, 2, 503],\n",
              " [4, 16, 1223, 3, 5, 2, 503, 715],\n",
              " [17, 906],\n",
              " [17, 906, 2006],\n",
              " [17, 906, 2006, 148],\n",
              " [17, 906, 2006, 148, 2007],\n",
              " [141, 10],\n",
              " [141, 10, 2008],\n",
              " [141, 10, 2008, 36],\n",
              " [141, 10, 2008, 36, 2],\n",
              " [141, 10, 2008, 36, 2, 2009],\n",
              " [141, 10, 2008, 36, 2, 2009, 5],\n",
              " [141, 10, 2008, 36, 2, 2009, 5, 2],\n",
              " [141, 10, 2008, 36, 2, 2009, 5, 2, 581],\n",
              " [107, 504],\n",
              " [107, 504, 12],\n",
              " [107, 504, 12, 286],\n",
              " [107, 504, 12, 286, 583],\n",
              " [107, 504, 12, 286, 583, 12],\n",
              " [107, 504, 12, 286, 583, 12, 505],\n",
              " [107, 504, 12, 286, 583, 12, 505, 174],\n",
              " [716, 34],\n",
              " [716, 34, 2010],\n",
              " [716, 34, 2010, 2011],\n",
              " [716, 34, 2010, 2011, 14],\n",
              " [716, 34, 2010, 2011, 14, 2012],\n",
              " [2, 1224],\n",
              " [2, 1224, 5],\n",
              " [2, 1224, 5, 2013],\n",
              " [2, 1224, 5, 2013, 2014],\n",
              " [2, 1224, 5, 2013, 2014, 39],\n",
              " [2, 1224, 5, 2013, 2014, 39, 125],\n",
              " [2, 1224, 5, 2013, 2014, 39, 125, 225],\n",
              " [3, 40],\n",
              " [3, 40, 56],\n",
              " [3, 40, 56, 95],\n",
              " [3, 40, 56, 95, 33],\n",
              " [3, 40, 56, 95, 33, 264],\n",
              " [3, 40, 56, 95, 33, 264, 133],\n",
              " [3, 40, 56, 95, 33, 264, 133, 387],\n",
              " [3, 40, 56, 95, 33, 264, 133, 387, 2015],\n",
              " [2, 693],\n",
              " [2, 693, 37],\n",
              " [2, 693, 37, 584],\n",
              " [2, 693, 37, 584, 40],\n",
              " [2, 693, 37, 584, 40, 33],\n",
              " [2, 693, 37, 584, 40, 33, 2016],\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we will make sure every above sentence are of equal length\n",
        "# apply pad sequences\n",
        "\n",
        "max_sequence_len=max([len(x) for x in input_sequences])\n",
        "input_sequences=np.array(pad_sequences(input_sequences,maxlen=max_sequence_len,padding='pre'))"
      ],
      "metadata": {
        "id": "SQw7cVGbqbfg"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5Tslh_1rW_i",
        "outputId": "12f8df41-201d-49a8-afff-19c92fc1a7bc"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ...,    0,    2,  688],\n",
              "       [   0,    0,    0, ...,    2,  688,    5],\n",
              "       [   0,    0,    0, ...,  688,    5,   46],\n",
              "       ...,\n",
              "       [   0,    0,    0, ...,    5,   46, 1048],\n",
              "       [   0,    0,    0, ...,   46, 1048,    5],\n",
              "       [   0,    0,    0, ..., 1048,    5,  194]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# therefore now wvery sentence has equal length"
      ],
      "metadata": {
        "id": "VSbIQiNTrYIA"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create predictors and labels\n",
        "import tensorflow as tf\n",
        "x,y=input_sequences[:,:-1],input_sequences[:,-1]\n",
        "# x= all the words except for last word\n",
        "# y=last word"
      ],
      "metadata": {
        "id": "EIQWinq-rrPN"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiTL9dQHsRY9",
        "outputId": "f722e3b4-be63-4e79-8c95-73c65d9b983e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ...,    0,    0,    2],\n",
              "       [   0,    0,    0, ...,    0,    2,  688],\n",
              "       [   0,    0,    0, ...,    2,  688,    5],\n",
              "       ...,\n",
              "       [   0,    0,    0, ...,  688,    5,   46],\n",
              "       [   0,    0,    0, ...,    5,   46, 1048],\n",
              "       [   0,    0,    0, ...,   46, 1048,    5]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x408UuFIsSNW",
        "outputId": "1d237c11-4cca-4af2-d7be-bfcb23a3d508"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 688,    5,   46, ..., 1048,    5,  194], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y=tf.keras.utils.to_categorical(y,num_classes=total_words)\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPVJJSWFsSuc",
        "outputId": "34428e0c-ed0e-4f4f-ed81-bab9a2a851b6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train the dataset\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "hdtpUPvBskt7"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train our lstm rnn\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout\n",
        "\n",
        "#define the model\n",
        "model=Sequential()\n",
        "model.add(Embedding(total_words,100,input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(150,return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words,activation='softmax'))\n",
        "\n",
        "#compile the model\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "QiO4Li6RtJmt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "outputId": "f23ecb80-bc8f-4bc5-9f21-7a53b0e5050a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "\n",
        "model.fit(x_train,y_train,epochs=50,batch_size=128,validation_data=(x_test,y_test),verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gF6ZKzI__KX6",
        "outputId": "aabe053b-dce6-4513-eea4-3e55323fc0b4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 200ms/step - accuracy: 0.0301 - loss: 7.4213 - val_accuracy: 0.0336 - val_loss: 6.7804\n",
            "Epoch 2/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 177ms/step - accuracy: 0.0345 - loss: 6.5508 - val_accuracy: 0.0385 - val_loss: 6.7303\n",
            "Epoch 3/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 174ms/step - accuracy: 0.0377 - loss: 6.4286 - val_accuracy: 0.0443 - val_loss: 6.7733\n",
            "Epoch 4/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 173ms/step - accuracy: 0.0463 - loss: 6.2618 - val_accuracy: 0.0449 - val_loss: 6.7710\n",
            "Epoch 5/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 177ms/step - accuracy: 0.0529 - loss: 6.1586 - val_accuracy: 0.0464 - val_loss: 6.7689\n",
            "Epoch 6/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 177ms/step - accuracy: 0.0517 - loss: 6.1191 - val_accuracy: 0.0497 - val_loss: 6.7938\n",
            "Epoch 7/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 176ms/step - accuracy: 0.0509 - loss: 6.0371 - val_accuracy: 0.0490 - val_loss: 6.7946\n",
            "Epoch 8/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 184ms/step - accuracy: 0.0562 - loss: 5.9988 - val_accuracy: 0.0482 - val_loss: 6.8203\n",
            "Epoch 9/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 176ms/step - accuracy: 0.0571 - loss: 5.9372 - val_accuracy: 0.0486 - val_loss: 6.8566\n",
            "Epoch 10/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 179ms/step - accuracy: 0.0638 - loss: 5.8701 - val_accuracy: 0.0507 - val_loss: 6.8620\n",
            "Epoch 11/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 187ms/step - accuracy: 0.0638 - loss: 5.8118 - val_accuracy: 0.0528 - val_loss: 6.8890\n",
            "Epoch 12/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 182ms/step - accuracy: 0.0662 - loss: 5.7477 - val_accuracy: 0.0534 - val_loss: 6.9234\n",
            "Epoch 13/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 180ms/step - accuracy: 0.0692 - loss: 5.6873 - val_accuracy: 0.0589 - val_loss: 6.9329\n",
            "Epoch 14/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 179ms/step - accuracy: 0.0773 - loss: 5.6101 - val_accuracy: 0.0579 - val_loss: 6.9652\n",
            "Epoch 15/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 178ms/step - accuracy: 0.0746 - loss: 5.5802 - val_accuracy: 0.0612 - val_loss: 6.9938\n",
            "Epoch 16/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 178ms/step - accuracy: 0.0830 - loss: 5.5128 - val_accuracy: 0.0598 - val_loss: 7.0199\n",
            "Epoch 17/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 194ms/step - accuracy: 0.0869 - loss: 5.4463 - val_accuracy: 0.0622 - val_loss: 7.0559\n",
            "Epoch 18/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 177ms/step - accuracy: 0.0932 - loss: 5.3744 - val_accuracy: 0.0635 - val_loss: 7.0772\n",
            "Epoch 19/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 174ms/step - accuracy: 0.0905 - loss: 5.3389 - val_accuracy: 0.0666 - val_loss: 7.0903\n",
            "Epoch 20/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 193ms/step - accuracy: 0.0972 - loss: 5.2802 - val_accuracy: 0.0664 - val_loss: 7.1404\n",
            "Epoch 21/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 174ms/step - accuracy: 0.1039 - loss: 5.2208 - val_accuracy: 0.0666 - val_loss: 7.1715\n",
            "Epoch 22/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 174ms/step - accuracy: 0.1057 - loss: 5.1509 - val_accuracy: 0.0697 - val_loss: 7.1868\n",
            "Epoch 23/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 178ms/step - accuracy: 0.1096 - loss: 5.1152 - val_accuracy: 0.0692 - val_loss: 7.2407\n",
            "Epoch 24/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 179ms/step - accuracy: 0.1125 - loss: 5.0749 - val_accuracy: 0.0699 - val_loss: 7.2286\n",
            "Epoch 25/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 178ms/step - accuracy: 0.1105 - loss: 5.0366 - val_accuracy: 0.0682 - val_loss: 7.2787\n",
            "Epoch 26/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 181ms/step - accuracy: 0.1215 - loss: 4.9557 - val_accuracy: 0.0688 - val_loss: 7.2894\n",
            "Epoch 27/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 179ms/step - accuracy: 0.1212 - loss: 4.9147 - val_accuracy: 0.0688 - val_loss: 7.3279\n",
            "Epoch 28/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 178ms/step - accuracy: 0.1214 - loss: 4.8892 - val_accuracy: 0.0690 - val_loss: 7.3669\n",
            "Epoch 29/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 180ms/step - accuracy: 0.1312 - loss: 4.8085 - val_accuracy: 0.0661 - val_loss: 7.3783\n",
            "Epoch 30/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.1285 - loss: 4.7957 - val_accuracy: 0.0664 - val_loss: 7.4168\n",
            "Epoch 31/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 177ms/step - accuracy: 0.1336 - loss: 4.7462 - val_accuracy: 0.0666 - val_loss: 7.4602\n",
            "Epoch 32/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 175ms/step - accuracy: 0.1379 - loss: 4.7087 - val_accuracy: 0.0678 - val_loss: 7.4848\n",
            "Epoch 33/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 174ms/step - accuracy: 0.1400 - loss: 4.6412 - val_accuracy: 0.0668 - val_loss: 7.5253\n",
            "Epoch 34/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 194ms/step - accuracy: 0.1460 - loss: 4.6113 - val_accuracy: 0.0707 - val_loss: 7.5386\n",
            "Epoch 35/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 175ms/step - accuracy: 0.1577 - loss: 4.5326 - val_accuracy: 0.0661 - val_loss: 7.5950\n",
            "Epoch 36/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 179ms/step - accuracy: 0.1539 - loss: 4.5292 - val_accuracy: 0.0661 - val_loss: 7.6272\n",
            "Epoch 37/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 178ms/step - accuracy: 0.1561 - loss: 4.4902 - val_accuracy: 0.0684 - val_loss: 7.6483\n",
            "Epoch 38/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 177ms/step - accuracy: 0.1598 - loss: 4.4445 - val_accuracy: 0.0663 - val_loss: 7.6812\n",
            "Epoch 39/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 180ms/step - accuracy: 0.1684 - loss: 4.4091 - val_accuracy: 0.0664 - val_loss: 7.7071\n",
            "Epoch 40/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 194ms/step - accuracy: 0.1687 - loss: 4.3604 - val_accuracy: 0.0668 - val_loss: 7.7437\n",
            "Epoch 41/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 177ms/step - accuracy: 0.1752 - loss: 4.3110 - val_accuracy: 0.0657 - val_loss: 7.7809\n",
            "Epoch 42/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 178ms/step - accuracy: 0.1798 - loss: 4.2706 - val_accuracy: 0.0653 - val_loss: 7.8244\n",
            "Epoch 43/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 177ms/step - accuracy: 0.1832 - loss: 4.2385 - val_accuracy: 0.0674 - val_loss: 7.8415\n",
            "Epoch 44/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 176ms/step - accuracy: 0.1913 - loss: 4.1892 - val_accuracy: 0.0657 - val_loss: 7.8990\n",
            "Epoch 45/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 176ms/step - accuracy: 0.1964 - loss: 4.1604 - val_accuracy: 0.0628 - val_loss: 7.9192\n",
            "Epoch 46/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 177ms/step - accuracy: 0.1998 - loss: 4.1239 - val_accuracy: 0.0631 - val_loss: 7.9564\n",
            "Epoch 47/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 177ms/step - accuracy: 0.2061 - loss: 4.0858 - val_accuracy: 0.0635 - val_loss: 7.9906\n",
            "Epoch 48/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 177ms/step - accuracy: 0.2122 - loss: 4.0494 - val_accuracy: 0.0629 - val_loss: 8.0312\n",
            "Epoch 49/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 179ms/step - accuracy: 0.2161 - loss: 3.9897 - val_accuracy: 0.0641 - val_loss: 8.0745\n",
            "Epoch 50/50\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 173ms/step - accuracy: 0.2224 - loss: 3.9709 - val_accuracy: 0.0645 - val_loss: 8.1065\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7cb7b72bdd10>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict the next word\n",
        "def predict_next_word(model, tokenizer, text, max_sequence_len):\n",
        "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
        "    if len(token_list) >= max_sequence_len:\n",
        "        token_list = token_list[-(max_sequence_len-1):]  # Ensure the sequence length matches max_sequence_len-1\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = model.predict(token_list, verbose=0)\n",
        "    predicted_word_index = np.argmax(predicted, axis=1)\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted_word_index:\n",
        "            return word\n",
        "    return None"
      ],
      "metadata": {
        "id": "Vu_-uPD9pHxO"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text=\" to be or not to be\"\n",
        "print(f\"input text:{input_text}\")\n",
        "max_sequence_len=model.input_shape[1]+1\n",
        "next_word=predict_next_word(model,tokenizer,input_text,max_sequence_len)\n",
        "print(f\"next word prediction:{next_word}\")"
      ],
      "metadata": {
        "id": "bZ1eGrOmppf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3bb2617-bc02-4d67-ff62-4831ea437ed5"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input text: to be or not to be\n",
            "next word prediction:against\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model\n",
        "model.save(\"next_word_lstm.h5\")\n",
        "\n",
        "# save the tokenizer\n",
        "import pickle\n",
        "with open('tokenizer.pickle','wb') as handle:\n",
        "  pickle.dump(tokenizer,handle,protocol=pickle.HIGHEST_PROTOCOL)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpniUVeujtz0",
        "outputId": "4457dcd2-b949-4264-ff27-2adc549eb8b0"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DEPLOYING WITH STREAMLIT\n",
        "\n",
        "!pip install streamlit\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tensorflow import keras\n",
        "from keras.utils import pad_sequences\n",
        "\n",
        "model=keras.models.load_model('next_word_lstm.h5')\n",
        "\n",
        "with open('tokenizer.pickle','rb') as handle:\n",
        "  tokenizer=pickle.load(handle)\n",
        "\n",
        "def predict_next_word(model, tokenizer, text, max_sequence_len):\n",
        "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
        "    if len(token_list) >= max_sequence_len:\n",
        "        token_list = token_list[-(max_sequence_len-1):]  # Ensure the sequence length matches max_sequence_len-1\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = model.predict(token_list, verbose=0)\n",
        "    predicted_word_index = np.argmax(predicted, axis=1)\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted_word_index:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "# streamlit app\n",
        "st.title(\"Next Word Prediction With LSTM And Early Stopping\")\n",
        "input_text=st.text_input(\"Enter the sequence of Words\",\"To be or not to\")\n",
        "if st.button(\"Predict Next Word\"):\n",
        "    max_sequence_len = model.input_shape[1] + 1  # Retrieve the max sequence length from the model input shape\n",
        "    next_word = predict_next_word(model, tokenizer, input_text, max_sequence_len)\n",
        "    st.write(f'Next word: {next_word}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CoTaU4WF1ih",
        "outputId": "e54bc518-d116-42da-dcda-0d3610853e09"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.44.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.35.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "2025-04-20 07:39:38.058 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-20 07:39:38.174 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-04-20 07:39:38.175 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-20 07:39:38.178 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-20 07:39:38.179 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-20 07:39:38.181 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-20 07:39:38.183 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-20 07:39:38.185 Session state does not function when running a script without `streamlit run`\n",
            "2025-04-20 07:39:38.186 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-20 07:39:38.188 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-20 07:39:38.189 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-20 07:39:38.191 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-20 07:39:38.191 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-20 07:39:38.192 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-20 07:39:38.193 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    }
  ]
}